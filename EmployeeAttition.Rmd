---
title: "HR Attrition"
output: html_notebook
---

# Introduction:

In this notebook I will be looking at IBM's employee attrition data obtained from Kaggle https://www.kaggle.com/pavansubhasht/ibm-hr-analytics-attrition-dataset

My goal in this notebook is to predict the likeklihood of employees leaving teh company and assessing the impact this will have on the company.
```{r}
library(tidyverse)
library(caret)
library(DataExplorer)
setwd("~/HRattrition")
df = read.csv("WA_Fn-UseC_-HR-Employee-Attrition 2.csv")
df$Over18 = NULL
df$EmployeeCount = NULL
df$StandardHours = NULL
```

# EDA
There are 35 variables and 1470 observations of these in this dataframe.  I will use the head() command to display the first few rows
```{r}
#taking a quick look at the data 
df %>% head()
```

The plot_histogram() function from DataExplorer is useful to plot the distributions of all numeric values in a dataframe.  One thing to note is that Age is the only variable that is nearing normality.

```{r}
df %>% plot_histogram()
```

There is no missing values in the whole dataset.  This is helpful because missing values are problematic when modeling.  

```{r}
df %>% plot_missing()
```

A quick correlation matrix of the numeric features higlights that WorklifeBalance, YearsAtCompany, YearsInCurrentRole, YearsSinceLastPromotion and YearsWithCurrentManager contain much of the same information and are highly correlated.

```{r}
df %>% select_if(is.numeric) %>%  plot_correlation(type = "c")
```

The varible of interest for prediction is _Attrition_ a 2 level factor that indicated whether turnover occured with the given observation.  I plot a simple barplot of the Attrition variable.


```{r}
df %>% ggplot(aes(Attrition)) +
  geom_bar()+
  ggtitle("Employee Turnover")
```
# Modeling 

The first model that I will fit is a random forest model using the randomForest package.  Random forests are my favorite model.  They are quite accurate and provide alot of important information about the relationship between the response variable and the predictors.

```{r}
# I import the caret library which I will use throughout this analysis 
library(caret)
# I define a 70 - 30 train test split in the data.
split = createDataPartition(df$Attrition, p = 0.7, list = F)
train = df[split,] 
test = df[-split,] 
```

The Random forest model is fairly accurate on the held out test set. It is noteable that specificity is quite low in the model.  This hihglights the fact the model is having a difficult time distinguishing between true and false negatives. This moodel does not do the best at predicting employees that do actually end up leaving the company.

```{r}
library(randomForest)

# I fit a random forest using the default parameters
rf =randomForest::randomForest(Attrition~., data = train)
pred = predict(rf, test, type = "response")
#pred[pred > 0.5] = "Yes"
#pred[pred < 0.5] = "No"
mean(pred == test$Attrition)
confusionMatrix(pred, test$Attrition)
```
```{r}
rf
```

According to the random forest model, the Monthly income and age of an employee are the most important indicators of turnover.

```{r}
plot1= varImpPlot(rf, n.var = 17)
```
```{r}
df %>% ggplot(aes(MonthlyIncome, DailyRate, color = Attrition))+
  geom_point()+
  geom_smooth(method = "lm")
```

```{r}
df$Attrition = as.factor(df$Attrition)
```


```{r}
library(pdp)
rf %>%
  partial(pred.var = "DailyRate") %>%
  plotPartial(rug = TRUE, train = train, main = "Monthly Income Partial Dependence Plot")
```

```{r}
library(pdp)
rf %>%
  partial(pred.var = "Age") %>%
  plotPartial(rug = TRUE, train = train, main = "Age Partial Dependence Plot")
```
```{r}
library(pdp)
rf %>%
  partial(pred.var = "OverTime") %>%
  plotPartial(rug = TRUE, train = train)
```






```{r}
set.seed(2017)
ctrl <- trainControl(method = "repeatedcv", repeats = 3)
regFit <- train(
  Attrition ~ .,
  data = train %>% drop_na(),
  trControl = ctrl,
  method = "glm",
  #family = "binomial",
  ## Center and scale the predictors for the training
  ## set and all future samples.
  preProc = c("center", "scale"),
  tuneLength = 10
)
```

```{r}
pred = predict(regFit, test, type = "prob")
pred2 = predict(regFit, test)
prop.table(confusionMatrix(pred2, test$Attrition)$table)
```

```{r}
plot(varImp(regFit),top = 15, main = "GLM variable importance", ylab = "Predictor")

```



```{r}
summary(regFit$finalModel)
```

```{r}
Fit <- train(
  Attrition ~ .,
  data = train %>% drop_na(),
  trControl = ctrl,
  method = "rf",
  #family = "binomial",
  ## Center and scale the predictors for the training
  ## set and all future samples.
  preProc = c("center", "scale"),
  tuneLength = 10
)
```

```{r}
confusionMatrix(Fit)
```


```{r}
plot(varImp(Fit), 15)
```

```{r}
df %>% ggplot(aes(EnvironmentSatisfaction, fill = OverTime))+
  geom_histogram(bins = 30)
```

```{r}
df %>% ggplot(aes(EmployeeSa, fill = OverTime))+
  geom_density()
```


